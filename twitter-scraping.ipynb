{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import GetOldTweets3 as got\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The function itself\n",
    "\n",
    "I'll probably nest this in a functions.py file in the final project so it doesn't take up notebook space, but leaving it here for now so you can look through it easily, if you'd like! Scroll to the bottom to use it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_tweets_to_csv(query:str, max_tweets:int, cities:dict, date_range:tuple, state:str, sleep_time:float=1.5):\n",
    "    '''\n",
    "    A function for returning search results on a query\n",
    "    to create a representative sample of a state/region\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query : string, a search query to be passed through\n",
    "    Twitter's advanced search. Can use booleans within\n",
    "    the query!\n",
    "    \n",
    "    max_tweets : int, number of tweets to pull, recommend\n",
    "    staying within the boundaries of the twitter API limitations\n",
    "    (recommend using 8000 as an upper limit)\n",
    "    \n",
    "    cities : dict, dictionary where the keys are [city, state abbreviation] \n",
    "    and the values are the distance around the city to search.\n",
    "    Keys should be strings, values can be strings or integers.\n",
    "    Not case-sensitive\n",
    "    Example: {'chicago': 10, 'sPringfield': '20'}\n",
    "    \n",
    "    date_range : tuple, a range of dates as stringts to pull \n",
    "    tweets from, formatted as 'YYYY-MM-DD'. Put earliest date first. \n",
    "    Example: ('2020-03-20', '2020-03-25')\n",
    "    \n",
    "    state : string, enter the two-letter state code you are pulling info from.\n",
    "    Not case-sensitive.\n",
    "    '''\n",
    "    # Makes the data folder in the directory if you don't already have it\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    \n",
    "    def csv_store(resultsAux):\n",
    "        '''\n",
    "        A function that is used within getTweets() as a receive buffer.\n",
    "        This function stores a city's info in a .csv so if you hit a\n",
    "        rate limit, your data gets saved.\n",
    "        '''\n",
    "        # Create dataframe from the temporary variable, resultsAux (comes from getTweets() source code)\n",
    "        df = pd.DataFrame(t.__dict__ for t in resultsAux)\n",
    "\n",
    "        # Is this the first city?\n",
    "        if city == list(cities.keys())[0]:\n",
    "            # Add city column to this df and write to new .csv, \n",
    "            # the .csv will be removed at the end of the whole function\n",
    "            df['city'] = city\n",
    "            df['query'] = query\n",
    "            df['date_range'] = str(date_range)\n",
    "            df.to_csv(f'./data/{city}_scrape_data.csv', index=False, mode='a')\n",
    "\n",
    "        else:\n",
    "            # Don't need header for anything but the first city\n",
    "            df['city'] = city\n",
    "            df['query'] = query\n",
    "            df['date_range'] = str(date_range)\n",
    "            df.to_csv(f'./data/{city}_scrape_data.csv', index=False, mode='a', header=False)\n",
    "            \n",
    "        \n",
    "    \n",
    "    # Create a static timestamp to use for versioning\n",
    "    timestamp = str(time.ctime().replace(' ', '_').replace(':', '_'))\n",
    "    \n",
    "    # Set state to uppercase for filenaming uniformity\n",
    "    state = state.upper()\n",
    "    \n",
    "    for city, area in cities.items():\n",
    "        # Make city lowercase for consitent file naming\n",
    "        city = city.lower()\n",
    "        \n",
    "        # Try to get all tweets as determined by max_tweets\n",
    "        try:\n",
    "            tweetCriteria = got.manager.TweetCriteria().setQuerySearch(query)\\\n",
    "                                               .setSince(date_range[0])\\\n",
    "                                               .setUntil(date_range[1])\\\n",
    "                                               .setMaxTweets(max_tweets)\\\n",
    "                                               .setNear(f'{city}, {state}')\\\n",
    "                                               .setWithin(f'{str(area)}mi')\n",
    "            tweets = got.manager.TweetManager.getTweets(tweetCriteria, \n",
    "                                                        receiveBuffer=csv_store) # This receive buffer goes into the csv_store function defined above\n",
    "\n",
    "            # Let's get the current city's csv that was created above\n",
    "            current_city = pd.read_csv(f'./data/{city}_scrape_data.csv')\n",
    "            \n",
    "            # Tell me how many tweets we collected\n",
    "            print(f'Finished collecting tweets from {city}, we got {len(current_city)} tweets')\n",
    "\n",
    "            # Is this the first city?\n",
    "            if city == list(cities.keys())[0]:\n",
    "                # Create a .csv and put each city's data inside\n",
    "                current_city.to_csv(f'./data/{state}_scrape_data_{timestamp}.csv', mode='a', index=False)\n",
    "                # Clean up the directory by removing the city's .csv\n",
    "                os.remove(f'./data/{city}_scrape_data.csv')\n",
    "                \n",
    "                # Rest a random amount to try not to be detected as a bot\n",
    "                time.sleep(np.random.normal(sleep_time, 0.1))\n",
    "            \n",
    "            # Is this the last city? Don't sleep after it!\n",
    "            elif city == list(cities.keys())[-1]:  \n",
    "                # Don't need header for anything but the first city\n",
    "                current_city.to_csv(f'./data/{state}_scrape_data_{timestamp}.csv', mode='a', index=False, header=False)\n",
    "                # Clean up the directory by removing the city's .csv\n",
    "                os.remove(f'./data/{city}_scrape_data.csv')             \n",
    "            \n",
    "            else:\n",
    "                # Don't need header for anything but the first city\n",
    "                current_city.to_csv(f'./data/{state}_scrape_data_{timestamp}.csv', mode='a', index=False, header=False)\n",
    "                os.remove(f'./data/{city}_scrape_data.csv')\n",
    "                \n",
    "                # Rest a random amount to try not to be detected as a bot\n",
    "                time.sleep(np.random.normal(sleep_time, 0.1))\n",
    "                \n",
    "           \n",
    "        \n",
    "        # If one of the searches didn't return anything, it won't create a .csv and will throw an error, let's account for that\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        \n",
    "        # This is just a general catch-all for any other issues (including timeouts)\n",
    "        except:\n",
    "        \n",
    "            # If there were errors above, we'll have to account for the missing .csvs with another try/except\n",
    "            try:\n",
    "                # Let's get the current city's csv that was created above\n",
    "                current_city = pd.read_csv(f'./data/{city}_scrape_data.csv')\n",
    "\n",
    "                # Tell me how many tweets we collected\n",
    "                print(f'Finished collecting tweets from {city}, we got {len(current_city)} tweets')\n",
    "\n",
    "                # Is this the first city?\n",
    "                if city == list(cities.keys())[0]:\n",
    "                    # Create a .csv and put each city's data inside\n",
    "                    current_city.to_csv(f'./data/{state}_scrape_data_{timestamp}.csv', mode='a', index=False)\n",
    "                    # Clean up the directory by removing the city's .csv\n",
    "                    os.remove(f'./data/{city}_scrape_data.csv')\n",
    "\n",
    "                else:\n",
    "                    # Don't need header for anything but the first city\n",
    "                    current_city.to_csv(f'./data/{state}_scrape_data_{timestamp}.csv', mode='a', index=False, header=False)\n",
    "                    os.remove(f'./data/{city}_scrape_data.csv')\n",
    "\n",
    "                    # Rest a random amount to try not to be detected as a bot\n",
    "                    time.sleep(np.random.normal(sleep_time, 0.1))\n",
    "            \n",
    "            # If the .csv didn't exist, just sleep and go on to the next city!\n",
    "            except:\n",
    "                time.sleep(np.random.normal(sleep_time, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use this area to collect tweets!\n",
    "\n",
    "I haven't been able to grab many tweets from rural areas in a short date range, thinking about expanding date range before and after an announcement so that non-urban areas are better represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ''\n",
    "max_tweets = 3600\n",
    "\n",
    "# Picking wider ranges for more rural areas, shallower ranges for cities, used google maps to try not to overlap, but we can also check for duplicates afterward.\n",
    "cities = {\n",
    "          'springfield': 10,\n",
    "          'chIcago': 10,\n",
    "          'kewanee': 30,\n",
    "          'rockford': 10,\n",
    "          'freeport': 20,\n",
    "          'vandalia': 50,\n",
    "          'vermont': 20,\n",
    "          'onarga': 20,\n",
    "          'dixon': 20,\n",
    "          'peoria': 10,\n",
    "          'marion': 30,\n",
    "          'marissa': 20,\n",
    "          'highland park': 13,\n",
    "          'gurnee': 10,\n",
    "          'round lake': 5,\n",
    "          'fox lake': 5,\n",
    "          'marengo': 10,\n",
    "          'galena': 2,\n",
    "          'sterling': 15,\n",
    "          'paw paw': 17,\n",
    "          'naperville': 3,\n",
    "          'aurora': 3,\n",
    "          'bolingbrook': 3,\n",
    "          'elgin': 10,\n",
    "          'bristol': 10,\n",
    "          'orland park': 5,\n",
    "          'blue island': 5,\n",
    "          'streator': 30,\n",
    "          'monmouth': 15,\n",
    "          'macomb': 13,\n",
    "          'ripley': 18,\n",
    "          'jacksonville': 2,\n",
    "          'san jose': 34,\n",
    "          'peoria': 5,\n",
    "          'farmington': 10,\n",
    "          'bloomington': 8,\n",
    "          'melvin': 25,\n",
    "          'champaign': 5,\n",
    "          'tuscola': 15,\n",
    "          'decatur': 5,\n",
    "          'island grove': 26,\n",
    "          'sumner': 10,\n",
    "          'oblong': 10,\n",
    "          'marshall': 2,\n",
    "          'paris': 3,\n",
    "          'danville': 2,\n",
    "          'fairfield': 16,\n",
    "          'mt carmel': 2,\n",
    "          'enfield': 8,\n",
    "          'harrisburg': 18,\n",
    "          'pleasant grove': 12,\n",
    "          'carbondale': 6,\n",
    "          'pickneyville': 15,\n",
    "          'hecker': 11,\n",
    "          'st jacob': 20,\n",
    "          'nokomis': 20\n",
    "         }\n",
    "date_range = ('2020-03-13', '2020-03-28')\n",
    "state = 'il'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished collecting tweets from springfield, we got 3635 tweets\n",
      "Finished collecting tweets from chicago, we got 3635 tweets\n",
      "Finished collecting tweets from kewanee, we got 146 tweets\n",
      "Finished collecting tweets from rockford, we got 3635 tweets\n",
      "Finished collecting tweets from freeport, we got 467 tweets\n",
      "Finished collecting tweets from vandalia, we got 158 tweets\n",
      "Finished collecting tweets from vermont, we got 39 tweets\n",
      "Finished collecting tweets from onarga, we got 71 tweets\n",
      "Finished collecting tweets from dixon, we got 892 tweets\n",
      "Finished collecting tweets from peoria, we got 2634 tweets\n",
      "Finished collecting tweets from marion, we got 2904 tweets\n",
      "Finished collecting tweets from marissa, we got 260 tweets\n",
      "Finished collecting tweets from highland park, we got 3635 tweets\n",
      "Finished collecting tweets from gurnee, we got 3635 tweets\n",
      "Finished collecting tweets from round lake, we got 3635 tweets\n",
      "Finished collecting tweets from fox lake, we got 3635 tweets\n",
      "Finished collecting tweets from marengo, we got 3635 tweets\n",
      "Finished collecting tweets from galena, we got 80 tweets\n",
      "Finished collecting tweets from sterling, we got 671 tweets\n",
      "Finished collecting tweets from paw paw, we got 73 tweets\n",
      "Finished collecting tweets from naperville, we got 3635 tweets\n",
      "Finished collecting tweets from aurora, we got 3635 tweets\n",
      "Finished collecting tweets from bolingbrook, we got 3635 tweets\n",
      "Finished collecting tweets from elgin, we got 3635 tweets\n",
      "Finished collecting tweets from bristol, we got 3635 tweets\n",
      "Finished collecting tweets from orland park, we got 3635 tweets\n",
      "Finished collecting tweets from blue island, we got 3635 tweets\n",
      "Finished collecting tweets from streator, we got 156 tweets\n",
      "Finished collecting tweets from monmouth, we got 896 tweets\n",
      "Finished collecting tweets from macomb, we got 722 tweets\n",
      "Finished collecting tweets from ripley, we got 3635 tweets\n",
      "Finished collecting tweets from jacksonville, we got 379 tweets\n",
      "Finished collecting tweets from san jose, we got 91 tweets\n",
      "Finished collecting tweets from farmington, we got 149 tweets\n",
      "Finished collecting tweets from bloomington, we got 3414 tweets\n",
      "Finished collecting tweets from melvin, we got 137 tweets\n",
      "Finished collecting tweets from champaign, we got 3635 tweets\n",
      "Finished collecting tweets from tuscola, we got 187 tweets\n",
      "Finished collecting tweets from decatur, we got 1696 tweets\n",
      "Finished collecting tweets from island grove, we got 3635 tweets\n",
      "Finished collecting tweets from sumner, we got 153 tweets\n",
      "Finished collecting tweets from oblong, we got 137 tweets\n",
      "Finished collecting tweets from marshall, we got 84 tweets\n",
      "Finished collecting tweets from paris, we got 39 tweets\n",
      "Finished collecting tweets from danville, we got 41 tweets\n",
      "Finished collecting tweets from fairfield, we got 73 tweets\n",
      "Finished collecting tweets from mt carmel, we got 83 tweets\n",
      "Finished collecting tweets from enfield, we got 24 tweets\n",
      "Finished collecting tweets from harrisburg, we got 256 tweets\n",
      "Finished collecting tweets from pleasant grove, we got 3635 tweets\n",
      "Finished collecting tweets from carbondale, we got 2381 tweets\n",
      "Finished collecting tweets from pickneyville, we got 3635 tweets\n",
      "Finished collecting tweets from hecker, we got 2026 tweets\n",
      "Finished collecting tweets from st jacob, we got 3635 tweets\n",
      "Finished collecting tweets from nokomis, we got 218 tweets\n"
     ]
    }
   ],
   "source": [
    "state_tweets_to_csv(query, max_tweets, cities, date_range, state, 180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = ''\n",
    "max_tweets = 3600\n",
    "\n",
    "# Picking wider ranges for more rural areas, shallower ranges for cities, used google maps to try not to overlap, but we can also check for duplicates afterward.\n",
    "cities = {\n",
    "          'resaca': 19,\n",
    "          'east ellijay': 14,\n",
    "          'cleveland': 30,\n",
    "          'nicholson': 14,\n",
    "          'athens': 6,\n",
    "          'carlton': 17,\n",
    "          'philomath': 31,\n",
    "          'grovetown': 15,\n",
    "          'Herndon': 25,\n",
    "          'glennville': 20,\n",
    "          'georgetown': 7,\n",
    "          'brunswick': 6,\n",
    "          'atkinson': 15,\n",
    "          'sunnyside': 18,\n",
    "          'douglas': 25,\n",
    "          'tifton': 18,\n",
    "          'rebecca': 8,\n",
    "          'pineview': 12,\n",
    "          'warner robins': 10,\n",
    "          'macon': 4,\n",
    "          'mcintyre': 10,\n",
    "          'deepstep': 15,\n",
    "          'round oak': 15,\n",
    "          'jersey': 16,\n",
    "          'windsor': 10,\n",
    "          'cumming': 13,\n",
    "          'atlanta': 5,\n",
    "          'cartersville': 21,\n",
    "          'temple': 16,\n",
    "          'hogansville': 20,\n",
    "          'waverly hall': 15,\n",
    "          'buena vista': 10,\n",
    "          'shellman': 15,\n",
    "          'albany': 5,\n",
    "          'rowena': 12,\n",
    "          'branchville': 20,\n",
    "          'bainbridge': 10,\n",
    "          'pavo': 20,\n",
    "          'valdosta': 8,\n",
    "          'dublin': 5,\n",
    "          'alamo': 5\n",
    "         }\n",
    "date_range = ('2020-03-25', '2020-04-09')\n",
    "state = 'ga'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished collecting tweets from resaca, we got 1541 tweets\n",
      "Finished collecting tweets from east ellijay, we got 220 tweets\n",
      "Finished collecting tweets from cleveland, we got 663 tweets\n",
      "Finished collecting tweets from nicholson, we got 3599 tweets\n",
      "An error occured during an HTTP request: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond\n",
      "Try to open in browser: https://twitter.com/search?q=%20near%3A%22athens%2C%20GA%22%20within%3A6mi%20since%3A2020-03-25%20until%3A2020-04-09&src=typd\n",
      "Finished collecting tweets from athens, we got 1299 tweets\n",
      "Finished collecting tweets from carlton, we got 503 tweets\n",
      "Finished collecting tweets from philomath, we got 295 tweets\n",
      "Finished collecting tweets from grovetown, we got 3599 tweets\n",
      "Finished collecting tweets from herndon, we got 3599 tweets\n",
      "Finished collecting tweets from glennville, we got 2892 tweets\n",
      "Finished collecting tweets from georgetown, we got 3599 tweets\n",
      "Finished collecting tweets from brunswick, we got 2716 tweets\n",
      "Finished collecting tweets from atkinson, we got 19 tweets\n",
      "Finished collecting tweets from sunnyside, we got 3599 tweets\n",
      "Finished collecting tweets from douglas, we got 363 tweets\n",
      "Finished collecting tweets from tifton, we got 671 tweets\n",
      "Finished collecting tweets from rebecca, we got 122 tweets\n",
      "Finished collecting tweets from pineview, we got 854 tweets\n",
      "Finished collecting tweets from warner robins, we got 3599 tweets\n",
      "Finished collecting tweets from macon, we got 3599 tweets\n",
      "Finished collecting tweets from mcintyre, we got 3599 tweets\n",
      "Finished collecting tweets from deepstep, we got 1137 tweets\n",
      "Finished collecting tweets from round oak, we got 1317 tweets\n",
      "Finished collecting tweets from jersey, we got 3599 tweets\n",
      "Finished collecting tweets from windsor, we got 3599 tweets\n",
      "Finished collecting tweets from cumming, we got 3599 tweets\n",
      "Finished collecting tweets from atlanta, we got 3599 tweets\n",
      "Finished collecting tweets from cartersville, we got 3599 tweets\n",
      "Finished collecting tweets from temple, we got 3599 tweets\n",
      "An error occured during an HTTP request: HTTP Error 429: Too Many Requests\n",
      "Try to open in browser: https://twitter.com/search?q=%20near%3A%22hogansville%2C%20GA%22%20within%3A20mi%20since%3A2020-03-25%20until%3A2020-04-09&src=typd\n",
      "Finished collecting tweets from hogansville, we got 3299 tweets\n",
      "Finished collecting tweets from waverly hall, we got 212 tweets\n",
      "Finished collecting tweets from buena vista, we got 11 tweets\n",
      "Finished collecting tweets from shellman, we got 145 tweets\n",
      "Finished collecting tweets from albany, we got 494 tweets\n",
      "Finished collecting tweets from rowena, we got 3599 tweets\n",
      "Finished collecting tweets from branchville, we got 3599 tweets\n",
      "Finished collecting tweets from bainbridge, we got 435 tweets\n",
      "Finished collecting tweets from pavo, we got 1027 tweets\n",
      "Finished collecting tweets from valdosta, we got 3599 tweets\n",
      "Finished collecting tweets from dublin, we got 3599 tweets\n"
     ]
    }
   ],
   "source": [
    "state_tweets_to_csv(query, max_tweets, cities, date_range, state, 180)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
