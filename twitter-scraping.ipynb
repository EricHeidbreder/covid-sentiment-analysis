{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import GetOldTweets3 as got\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The function itself\n",
    "\n",
    "I'll probably nest this in a functions.py file in the final project so it doesn't take up notebook space, but leaving it here for now so you can look through it easily, if you'd like! Scroll to the bottom to use it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_tweets_to_csv(query:str, tweets_per_iter:int, cities:dict, date_range:tuple, state:str):\n",
    "    '''\n",
    "    A function for returning search results on a query\n",
    "    to create a representative sample of a state/region\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query : string, a search query to be passed through\n",
    "    Twitter's advanced search. Can use booleans within\n",
    "    the query!\n",
    "    \n",
    "    tweets_per_iter : int, number of tweets to pull per iteration, \n",
    "    this is used to calculate the sleep time, may cause errors if using\n",
    "    a number greater than \n",
    "    (recommend using 8000 as an upper limit)\n",
    "    \n",
    "    cities : dict, dictionary where the keys are [city, state abbreviation] \n",
    "    and the values are the distance around the city to search.\n",
    "    Keys should be strings, values can be strings or integers.\n",
    "    Not case-sensitive\n",
    "    Example: {'chicago': 10, 'sPringfield': '20'}\n",
    "    \n",
    "    date_range : tuple, a range of dates as stringts to pull \n",
    "    tweets from, formatted as 'YYYY-MM-DD'. Put earliest date first. \n",
    "    Example: ('2020-03-20', '2020-03-25')\n",
    "    \n",
    "    state : string, enter the two-letter state code you are pulling info from.\n",
    "    Not case-sensitive.\n",
    "    '''\n",
    "    # Check to make sure we won't trigger a timeout on Twitter\n",
    "    if tweets_per_iter > 17999:\n",
    "        raise Exception(\"Your max Tweet per iter must 17999 or lower\")\n",
    "    \n",
    "    \n",
    "    # Makes the data folder in the directory if you don't already have it\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    \n",
    "    def csv_store(resultsAux):\n",
    "        '''\n",
    "        A function that is used within getTweets() as a receive buffer.\n",
    "        This function stores a city's info in a .csv so if you hit a\n",
    "        rate limit, your data gets saved.\n",
    "        '''\n",
    "        # Create dataframe from the temporary variable, resultsAux (comes from getTweets() source code)\n",
    "        df = pd.DataFrame(t.__dict__ for t in resultsAux)\n",
    "        \n",
    "        def add_cols():\n",
    "            # Add new columsn to the df\n",
    "            df['city'] = city\n",
    "            df['query'] = query\n",
    "            df['date_range'] = str(date_range)\n",
    "            df['state'] = state\n",
    "            df['date'] = pd.to_datetime(df['date'], utc=True)\n",
    "            df['month'] = df['date'].dt.month\n",
    "            df['day'] = df['date'].dt.day\n",
    "            \n",
    "        # Is this the first city?\n",
    "        if city == list(cities.keys())[0]:\n",
    "            # Add new columns to the df and write to new .csv\n",
    "            add_cols()\n",
    "            # the .csv will be removed at the end of the whole function\n",
    "            df.to_csv(f'./data/{city}_scrape_data.csv', index=False, mode='a')\n",
    "\n",
    "        else:\n",
    "            add_cols()\n",
    "            # Don't need header for anything but the first city\n",
    "            df.to_csv(f'./data/{city}_scrape_data.csv', index=False, mode='a', header=False)\n",
    "            \n",
    "    # Create a static timestamp to use for versioning\n",
    "    timestamp = str(time.ctime().replace(' ', '_').replace(':', '_'))\n",
    "    \n",
    "    # Set state to uppercase for filenaming uniformity\n",
    "    state = state.upper()\n",
    "    \n",
    "    \n",
    "    #-----------------------------------------------------------------------------\n",
    "    # Main search loop, developed by Eric Heidbreder, Haley Taft, Irene Anibogwu, and Steven Markoe\n",
    "    \n",
    "    # First, we need to set some variables and constants\n",
    "    max_id = 1295148306117476352 # Set a starting max_id, thanks Steven Markoe for figuring out this approach!\n",
    "    \n",
    "    time_window = 810 # Twitter's request timer resets every 15 minutes, shaving off 1% to be safe, as tweets_per_iter seems to have some variation.\n",
    "    max_tweets_per_time_window = 17999\n",
    "    sleep_time = time_window * (tweets_per_iter / max_tweets_per_time_window)\n",
    "    \n",
    "    for city, area in cities.items():\n",
    "        while True:\n",
    "            # Make city lowercase for consitent file naming\n",
    "            city = city.lower()\n",
    "\n",
    "            # Try to get all tweets as determined by tweets_per_iter\n",
    "\n",
    "            try:\n",
    "                tweetCriteria = got.manager.TweetCriteria().setQuerySearch(f'{query} max_id:{max_id}')\\\n",
    "                                                   .setSince(date_range[0])\\\n",
    "                                                   .setUntil(date_range[1])\\\n",
    "                                                   .setMaxTweets(tweets_per_iter)\\\n",
    "                                                   .setNear(f'{city}, {state}')\\\n",
    "                                                   .setEmoji('unicode')\\\n",
    "                                                   .setWithin(f'{str(area)}mi')\n",
    "                tweets = got.manager.TweetManager.getTweets(tweetCriteria, \n",
    "                                                            receiveBuffer=csv_store) # This receive buffer goes into the csv_store function defined above\n",
    "\n",
    "                # Let's get the current city's csv that was created from the getTweets() receiveBuffer\n",
    "                current_city = pd.read_csv(f'./data/{city}_scrape_data.csv')\n",
    "                \n",
    "                # Is this a full page of tweets? If not it means it's the last page\n",
    "                if len(current_city) < (tweets_per_iter / 2):\n",
    "                    print(f'Returned {len(current_city)} tweets, wrapping up work on {city}!')\n",
    "                    # Save this data to the csv\n",
    "                    current_city.to_csv(f'./data/{state}_scrape_data_{timestamp}.csv', mode='a', index=False, header=False)\n",
    "                    # Clean up the directory\n",
    "                    os.remove(f'./data/{city}_scrape_data.csv')\n",
    "                    break\n",
    "                \n",
    "                # Tell me how many tweets we collected\n",
    "                print(f'Finished current iteration for {city}, we got {len(current_city)} tweets.')\n",
    "                print(f'Waiting {sleep_time} seconds before next iteration')\n",
    "\n",
    "                max_id = int(current_city.tail(1)['id'].values[0]) # HALEY TAFT FIGURED THIS OUT!\n",
    "\n",
    "                # Is this the first city?\n",
    "                if city == list(cities.keys())[0]:\n",
    "                    # Create a .csv and put each city's data inside\n",
    "                    current_city.to_csv(f'./data/{state}_scrape_data_{timestamp}.csv', mode='a', index=False)\n",
    "                    # Clean up the directory by removing the city's .csv\n",
    "                    os.remove(f'./data/{city}_scrape_data.csv')\n",
    "\n",
    "                    # Rest a random amount to try not to be detected as a bot\n",
    "                    time.sleep(np.random.normal(sleep_time, 0.1))\n",
    "\n",
    "                    # Set the new max id to the last id in our previous dataframe. This will start the new query at this id.\n",
    "                    max_id = int(current_city.tail(1)['id'].values[0])\n",
    "\n",
    "                # Is this the last city? Don't sleep after it!\n",
    "                elif city == list(cities.keys())[-1]:  \n",
    "                    # Don't need header for anything but the first city\n",
    "                    current_city.to_csv(f'./data/{state}_scrape_data_{timestamp}.csv', mode='a', index=False, header=False)\n",
    "                    # Clean up the directory by removing the city's .csv\n",
    "                    os.remove(f'./data/{city}_scrape_data.csv') \n",
    "\n",
    "                    max_id = int(current_city.tail(1)['id'].values[0])\n",
    "\n",
    "                else:\n",
    "                    # Don't need header for anything but the first city\n",
    "                    current_city.to_csv(f'./data/{state}_scrape_data_{timestamp}.csv', mode='a', index=False, header=False)\n",
    "                    os.remove(f'./data/{city}_scrape_data.csv')\n",
    "\n",
    "                    # Rest a random amount to try not to be detected as a bot\n",
    "                    time.sleep(np.random.normal(sleep_time, 0.1))\n",
    "\n",
    "                    max_id = int(current_city.tail(1)['id'].values[0])\n",
    "\n",
    "\n",
    "\n",
    "            # If one of the searches didn't return anything, it won't create a .csv and will throw an error, let's account for that\n",
    "            except FileNotFoundError:\n",
    "                print(f'Found no tweets remaining for {city}, moving on to next city!')\n",
    "                break\n",
    "\n",
    "            # This is just a general catch-all for any other issues (including timeouts)\n",
    "            except:\n",
    "\n",
    "                # If there were errors above, we'll have to account for the missing .csvs with another try/except\n",
    "                try:\n",
    "                    # Let's get the current city's csv that was created above\n",
    "                    current_city = pd.read_csv(f'./data/{city}_scrape_data.csv')\n",
    "\n",
    "                    # Tell me how many tweets we collected\n",
    "                    print(f'Encountered error, storing {len(current_city)} tweets from {city} and moving on.')\n",
    "                    \n",
    "                    # Is this a full page of tweets? If not, it means it's the last page\n",
    "                    if len(current_city) < (tweets_per_iter / 2): \n",
    "                        current_city.to_csv(f'./data/{state}_scrape_data_{timestamp}.csv', mode='a', index=False, header=False)\n",
    "                        os.remove(f'./data/{city}_scrape_data.csv')\n",
    "                        break\n",
    "\n",
    "                    max_id = int(current_city.tail(1)['id'].values[0])\n",
    "\n",
    "                    # Is this the first city?\n",
    "                    if city == list(cities.keys())[0]:\n",
    "                        # Create a .csv and put each city's data inside\n",
    "                        current_city.to_csv(f'./data/{state}_scrape_data_{timestamp}.csv', mode='a', index=False)\n",
    "                        # Clean up the directory by removing the city's .csv\n",
    "                        os.remove(f'./data/{city}_scrape_data.csv')\n",
    "\n",
    "                    else:\n",
    "                        # Don't need header for anything but the first city\n",
    "                        current_city.to_csv(f'./data/{state}_scrape_data_{timestamp}.csv', mode='a', index=False, header=False)\n",
    "                        os.remove(f'./data/{city}_scrape_data.csv')\n",
    "\n",
    "                        # Rest a random amount to try not to be detected as a bot\n",
    "                        time.sleep(np.random.normal(sleep_time, 0.1))\n",
    "\n",
    "                # If the .csv didn't exist, just sleep and go on to the next city!\n",
    "                except:\n",
    "                    time.sleep(np.random.normal(sleep_time, 0.1))\n",
    "                    break\n",
    "    \n",
    "    try:\n",
    "        # Clean up final df\n",
    "        df_full = pd.read_csv(f'./data/{state}_scrape_data_{timestamp}.csv')\n",
    "        df_full = df_full[(df_full['username'] != 'username')]\n",
    "        df_full.dropna(subset=['text', 'date']) # There were some nulls in the text and date column that are likely the result of deleted/private tweets\n",
    "    except:\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking a search query\n",
    "\n",
    "We tried to **grab every tweet from each city**, but soon found that we wouldn't be able to do that (returned 20000 tweets from Chicago for just one day, and we want to search across a **2-week period**). We're also limited to the **number of terms** we can include in a query (testing showed it was somewhere around 25-40 words max, may be character based). We chose to build our query with words derived from the `top_words_il` dataframe, which contains the top words from Illinois after running the corpus through `CountVectorizer`. Illinois happened to be the first large dataframe we performed EDA on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_il = pd.read_csv('./data/top_words_cvec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_list = top_words_il.head(20)['0'].tolist()\n",
    "term_list.remove('illinois')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our query contains **24 terms**: \n",
    "* 50% are neutral words with stop words removed\n",
    "* 50% are Covid-19-related terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_list = ['get', 'one', 'time', 'people', 'day', 'know', 'today', 'need', 'go', 'home', 'right', 'going', \n",
    "             'pandemic', 'coronavirus', 'news', 'health', 'covid19', '19', 'quarantine', 'governor', 'capitol', 'capital', 'corona', 'virus'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use this area to collect tweets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Illinois query\n",
    "query = ' OR '.join(term_list) # Joining with OR so that we get tweets that contain those individual words rather than phrases\n",
    "tweets_per_iter = 3600\n",
    "\n",
    "# Picking wider ranges for more rural areas, shallower ranges for cities, used google maps to try not to overlap, but we can also check for duplicates afterward.\n",
    "cities = {\n",
    "          'springfield': 10,\n",
    "          'chIcago': 10,\n",
    "          'kewanee': 30,\n",
    "          'rockford': 10,\n",
    "          'freeport': 20,\n",
    "          'vandalia': 50,\n",
    "          'vermont': 20,\n",
    "          'onarga': 20,\n",
    "          'dixon': 20,\n",
    "          'peoria': 10,\n",
    "          'marion': 30,\n",
    "          'marissa': 20,\n",
    "          'highland park': 13,\n",
    "          'gurnee': 10,\n",
    "          'round lake': 5,\n",
    "          'fox lake': 5,\n",
    "          'marengo': 10,\n",
    "          'galena': 2,\n",
    "          'sterling': 15,\n",
    "          'paw paw': 17,\n",
    "          'naperville': 3,\n",
    "          'aurora': 3,\n",
    "          'bolingbrook': 3,\n",
    "          'elgin': 10,\n",
    "          'bristol': 10,\n",
    "          'orland park': 5,\n",
    "          'blue island': 5,\n",
    "          'streator': 30,\n",
    "          'monmouth': 15,\n",
    "          'macomb': 13,\n",
    "          'ripley': 18,\n",
    "          'jacksonville': 2,\n",
    "          'san jose': 34,\n",
    "          'peoria': 5,\n",
    "          'farmington': 10,\n",
    "          'bloomington': 8,\n",
    "          'melvin': 25,\n",
    "          'champaign': 5,\n",
    "          'tuscola': 15,\n",
    "          'decatur': 5,\n",
    "          'island grove': 26,\n",
    "          'sumner': 10,\n",
    "          'oblong': 10,\n",
    "          'marshall': 2,\n",
    "          'paris': 3,\n",
    "          'danville': 2,\n",
    "          'fairfield': 16,\n",
    "          'mt carmel': 2,\n",
    "          'enfield': 8,\n",
    "          'harrisburg': 18,\n",
    "          'pleasant grove': 12,\n",
    "          'carbondale': 6,\n",
    "          'pickneyville': 15,\n",
    "          'hecker': 11,\n",
    "          'st jacob': 20,\n",
    "          'nokomis': 20\n",
    "         }\n",
    "date_range = ('2020-03-17', '2020-03-31') # 2 weeks total, starting 3 days before governor announced state shutdown. The 'Until' date is exclusive, so this range looks like 15 days\n",
    "state = 'il'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to run\n",
    "# state_tweets_to_csv(query, tweets_per_iter, cities, date_range, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Georgia Query\n",
    "query = ' OR '.join(term_list) # Joining with OR so that we get tweets that contain those individual words rather than phrases\n",
    "tweets_per_iter = 3600\n",
    "\n",
    "# Picking wider ranges for more rural areas, shallower ranges for cities, used google maps to try not to overlap, but we can also check for duplicates afterward.\n",
    "cities = {\n",
    "          'resaca': 19,\n",
    "          'east ellijay': 14,\n",
    "          'cleveland': 30,\n",
    "          'nicholson': 14,\n",
    "          'athens': 6,\n",
    "          'carlton': 17,\n",
    "          'philomath': 31,\n",
    "          'grovetown': 15,\n",
    "          'Herndon': 25,\n",
    "          'glennville': 20,\n",
    "          'georgetown': 7,\n",
    "          'brunswick': 6,\n",
    "          'atkinson': 15,\n",
    "          'sunnyside': 18,\n",
    "          'douglas': 25,\n",
    "          'tifton': 18,\n",
    "          'rebecca': 8,\n",
    "          'pineview': 12,\n",
    "          'warner robins': 10,\n",
    "          'macon': 4,\n",
    "          'mcintyre': 10,\n",
    "          'deepstep': 15,\n",
    "          'round oak': 15,\n",
    "          'jersey': 16,\n",
    "          'windsor': 10,\n",
    "          'cumming': 13,\n",
    "          'atlanta': 5,\n",
    "          'cartersville': 21,\n",
    "          'temple': 16,\n",
    "          'hogansville': 20,\n",
    "          'waverly hall': 15,\n",
    "          'buena vista': 10,\n",
    "          'shellman': 15,\n",
    "          'albany': 5,\n",
    "          'rowena': 12,\n",
    "          'branchville': 20,\n",
    "          'bainbridge': 10,\n",
    "          'pavo': 20,\n",
    "          'valdosta': 8,\n",
    "          'dublin': 5,\n",
    "          'alamo': 5\n",
    "         }\n",
    "date_range = ('2020-03-30', '2020-04-13') # 2 weeks total, starting 3 days before governor announced state shutdown. The 'Until' date is exclusive, so this range looks like 15 days\n",
    "state = 'ga'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned 318 tweets, wrapping up work on resaca!\n",
      "Returned 72 tweets, wrapping up work on east ellijay!\n",
      "Returned 178 tweets, wrapping up work on cleveland!\n"
     ]
    }
   ],
   "source": [
    "# # uncomment to run\n",
    "# state_tweets_to_csv(query, tweets_per_iter, cities, date_range, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
