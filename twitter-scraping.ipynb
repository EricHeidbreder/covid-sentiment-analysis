{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import GetOldTweets3 as got\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The function itself\n",
    "\n",
    "I'll probably nest this in a functions.py file in the final project so it doesn't take up notebook space, but leaving it here for now so you can look through it easily, if you'd like! Scroll to the bottom to use it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_tweets_to_csv(query:str, tweets_per_iter:int, cities:dict, date_range:tuple, state:str):\n",
    "    '''\n",
    "    A function for returning search results on a query\n",
    "    to create a representative sample of a state/region\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    query : string, a search query to be passed through\n",
    "    Twitter's advanced search. Can use booleans within\n",
    "    the query!\n",
    "    \n",
    "    tweets_per_iter : int, number of tweets to pull per iteration, \n",
    "    this is used to calculate the sleep time, may cause errors if using\n",
    "    a number greater than \n",
    "    (recommend using 8000 as an upper limit)\n",
    "    \n",
    "    cities : dict, dictionary where the keys are [city, state abbreviation] \n",
    "    and the values are the distance around the city to search.\n",
    "    Keys should be strings, values can be strings or integers.\n",
    "    Not case-sensitive\n",
    "    Example: {'chicago': 10, 'sPringfield': '20'}\n",
    "    \n",
    "    date_range : tuple, a range of dates as stringts to pull \n",
    "    tweets from, formatted as 'YYYY-MM-DD'. Put earliest date first. \n",
    "    Example: ('2020-03-20', '2020-03-25')\n",
    "    \n",
    "    state : string, enter the two-letter state code you are pulling info from.\n",
    "    Not case-sensitive.\n",
    "    '''\n",
    "    # Cleaning cities variable\n",
    "    cities = {key.lower(): value for key, value in cities.items()}\n",
    "    \n",
    "    # Check to make sure we won't trigger a timeout on Twitter\n",
    "    if tweets_per_iter > 17999:\n",
    "        raise Exception(\"Your max Tweet per iter must 17999 or lower\")\n",
    "    \n",
    "    \n",
    "    # Makes the data folder in the directory if you don't already have it\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    \n",
    "    def csv_store(resultsAux):\n",
    "        '''\n",
    "        A function that is used within getTweets() as a receive buffer.\n",
    "        This function stores a city's info in a .csv so if you hit a\n",
    "        rate limit, your data gets saved.\n",
    "        '''\n",
    "        # Create dataframe from the temporary variable, resultsAux (comes from getTweets() source code)\n",
    "        df = pd.DataFrame(t.__dict__ for t in resultsAux)\n",
    "            \n",
    "        # Add new columns to the df and write to new .csv\n",
    "        df['city'] = city\n",
    "        df['query'] = query\n",
    "        df['date_range'] = str(date_range)\n",
    "        df['state'] = state\n",
    "        df['date'] = pd.to_datetime(df['date'], utc=True)\n",
    "        df['month'] = df['date'].dt.month\n",
    "        df['day'] = df['date'].dt.day\n",
    "        # the .csv and the extra headers will be removed at the end of the whole function\n",
    "        df.to_csv(f'./data/{city}_scrape_data.csv', index=False, mode='a')\n",
    "            \n",
    "    # Create a static timestamp to use for versioning\n",
    "    timestamp = str(time.ctime().replace(' ', '_').replace(':', '_'))\n",
    "    \n",
    "    # Set state to uppercase for filenaming uniformity\n",
    "    state = state.upper()\n",
    "    \n",
    "    \n",
    "    #-----------------------------------------------------------------------------\n",
    "    # Main search loop, developed by Eric Heidbreder, Haley Taft, Irene Anibogwu, and Steven Markoe\n",
    "    \n",
    "    '''\n",
    "    Setting Variables:\n",
    "    \n",
    "    Twitter's request timer resets every 15 minutes, \n",
    "    we add 3% in the sleep_time calculation to be safe, \n",
    "    as tweets_per_iter seems to have some variation.\n",
    "    '''\n",
    "    time_window = 900 \n",
    "    max_tweets_per_time_window = 17999\n",
    "    sleep_time = (time_window * 1.03) * (tweets_per_iter / max_tweets_per_time_window)\n",
    "    \n",
    "    for city, area in cities.items():\n",
    "         \n",
    "        # First, we need to set the max_id to a specific tweet earlier than all those we'll be searching for\n",
    "        max_id = 1295148306117476352 # Set a starting max_id, thanks Steven Markoe for figuring out this approach!\n",
    "        \n",
    "        while True:\n",
    "            # Start with a random rest so we don't trigger the search limit\n",
    "            print(f'Waiting {sleep_time} seconds before next iteration')\n",
    "            time.sleep(np.random.normal(sleep_time, 0.1))\n",
    "\n",
    "            # Try to get all tweets as determined by tweets_per_iter\n",
    "            try:\n",
    "                tweetCriteria = got.manager.TweetCriteria().setQuerySearch(f'{query} max_id:{max_id}')\\\n",
    "                                                   .setSince(date_range[0])\\\n",
    "                                                   .setUntil(date_range[1])\\\n",
    "                                                   .setMaxTweets(tweets_per_iter)\\\n",
    "                                                   .setNear(f'{city}, {state}')\\\n",
    "                                                   .setEmoji('unicode')\\\n",
    "                                                   .setWithin(f'{str(area)}mi')\n",
    "                tweets = got.manager.TweetManager.getTweets(tweetCriteria, \n",
    "                                                            receiveBuffer=csv_store) # This receive buffer goes into the csv_store function defined above\n",
    "\n",
    "                # Let's get the current city's csv that was created from the getTweets() receiveBuffer\n",
    "                current_city = pd.read_csv(f'./data/{city}_scrape_data.csv')\n",
    "                \n",
    "                # Is this a full page of tweets? If not it means it's the last page\n",
    "                if len(current_city) < (tweets_per_iter / 2):\n",
    "                    print(f'Returned {len(current_city)} tweets, wrapping up work on {city}!')\n",
    "                    # Save this data to the csv\n",
    "                    current_city.to_csv(f'./data/{state}_scrape_data_{timestamp}.csv', mode='a', index=False)\n",
    "                    # Clean up the directory\n",
    "                    os.remove(f'./data/{city}_scrape_data.csv')\n",
    "                    break\n",
    "                \n",
    "                # Tell me how many tweets we collected\n",
    "                print(f'Finished current iteration for {city}, we got {len(current_city)} tweets.')\n",
    "\n",
    "                max_id = int(current_city.tail(1)['id'].values[0]) # HALEY TAFT FIGURED THIS OUT!\n",
    "\n",
    "                # Create a .csv and put each city's data inside\n",
    "                current_city.to_csv(f'./data/{state}_scrape_data_{timestamp}.csv', mode='a', index=False)\n",
    "                # Clean up the directory by removing the city's .csv\n",
    "                os.remove(f'./data/{city}_scrape_data.csv')\n",
    "\n",
    "                # Set the new max id to the last id in our previous dataframe. This will start the new query at this id.\n",
    "                max_id = int(current_city.tail(1)['id'].values[0])\n",
    "\n",
    "            # If one of the searches didn't return anything, it won't create a .csv and will throw an error, let's account for that\n",
    "            except FileNotFoundError:\n",
    "                print(f'Found no tweets remaining for {city}, moving on to next city!')\n",
    "                break\n",
    "\n",
    "            # This is just a general catch-all for any other issues (including timeouts)\n",
    "            except:\n",
    "\n",
    "                # If there were errors above, we'll have to account for the missing .csv with another try/except\n",
    "                try:\n",
    "                    # Let's get the current city's csv that was created above\n",
    "                    current_city = pd.read_csv(f'./data/{city}_scrape_data.csv')\n",
    "\n",
    "                    # Tell me how many tweets we collected\n",
    "                    print(f'Encountered error, storing {len(current_city)} tweets from {city} and moving on.')\n",
    "                    \n",
    "                    # Is this a full page of tweets? If not, it means it's the last page\n",
    "                    if len(current_city) < (tweets_per_iter / 2): \n",
    "                        current_city.to_csv(f'./data/{state}_scrape_data_{timestamp}.csv', mode='a', index=False)\n",
    "                        os.remove(f'./data/{city}_scrape_data.csv')\n",
    "                        break\n",
    "\n",
    "                    max_id = int(current_city.tail(1)['id'].values[0])\n",
    "\n",
    "                    # Create a .csv and put each city's data inside\n",
    "                    current_city.to_csv(f'./data/{state}_scrape_data_{timestamp}.csv', mode='a', index=False)\n",
    "                    # Clean up the directory by removing the city's .csv\n",
    "                    os.remove(f'./data/{city}_scrape_data.csv')\n",
    "\n",
    "                # If the .csv didn't exist, just sleep and go on to the next city!\n",
    "                except:\n",
    "                    break\n",
    "    \n",
    "    try:\n",
    "        # Clean up final df\n",
    "        print('Cleaning up final dataframe!')\n",
    "        df_full = pd.read_csv(f'./data/{state}_scrape_data_{timestamp}.csv')\n",
    "        df_full = df_full[df_full['username'] != 'username'] # Removes headers leftover from scraping\n",
    "        df_full.dropna(subset=['text', 'date'], inplace=True) # There were some nulls in the text and date column that are likely the result of deleted/private tweets\n",
    "        df_full.to_csv(f'./data/{state}_scrape_data_{timestamp}.csv', index=False)\n",
    "    except:\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking a search query\n",
    "\n",
    "We tried to **grab every tweet from each city**, but soon found that we wouldn't be able to do that (returned 20000 tweets from Chicago for just one day, and we want to search across a **2-week period**). We're also limited to the **number of terms** we can include in a query (testing showed it was somewhere around 25-40 words max, may be character based). We chose to build our query with words derived from the `top_words_il` dataframe, which contains the top words from Illinois after running the corpus through `CountVectorizer`. Illinois happened to be the first large dataframe we performed EDA on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_il = pd.read_csv('./data/top_words_cvec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_list = top_words_il.head(20)['0'].tolist()\n",
    "term_list.remove('illinois')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our query contains **24 terms**: \n",
    "* 50% are neutral words with stop words removed\n",
    "* 50% are Covid-19-related terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "il_term_list = ['get', 'one', 'time', 'people', 'day', 'know', 'today', 'need', 'go', 'home', 'right', 'going', \n",
    "             'pandemic', 'coronavirus', 'news', 'health', 'covid', '19', 'quarantine', 'governor', 'capitol', 'capital', 'pritzker', 'virus'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use this area to collect tweets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Illinois query\n",
    "query = ' OR '.join(il_term_list) # Joining with OR so that we get tweets that contain those individual words rather than phrases\n",
    "tweets_per_iter = 1000\n",
    "\n",
    "# Picking wider ranges for more rural areas, shallower ranges for cities, used google maps to try not to overlap, but we can also check for duplicates afterward.\n",
    "cities = {\n",
    "          'springfield': 10,\n",
    "          'chicago': 10,\n",
    "          'kewanee': 30,\n",
    "          'rockford': 10,\n",
    "          'freeport': 20,\n",
    "          'vandalia': 50,\n",
    "          'vermont': 20,\n",
    "          'onarga': 20,\n",
    "          'dixon': 20,\n",
    "          'peoria': 10,\n",
    "          'marion': 30,\n",
    "          'marissa': 20,\n",
    "          'highland park': 13,\n",
    "          'gurnee': 10,\n",
    "          'round lake': 5,\n",
    "          'fox lake': 5,\n",
    "          'marengo': 10,\n",
    "          'galena': 2,\n",
    "          'sterling': 15,\n",
    "          'paw paw': 17,\n",
    "          'naperville': 3,\n",
    "          'aurora': 3,\n",
    "          'bolingbrook': 3,\n",
    "          'elgin': 10,\n",
    "          'bristol': 10,\n",
    "          'orland park': 5,\n",
    "          'blue island': 5,\n",
    "          'streator': 30,\n",
    "          'monmouth': 15,\n",
    "          'macomb': 13,\n",
    "          'ripley': 18,\n",
    "          'jacksonville': 2,\n",
    "          'san jose': 34,\n",
    "          'peoria': 5,\n",
    "          'farmington': 10,\n",
    "          'bloomington': 8,\n",
    "          'melvin': 25,\n",
    "          'champaign': 5,\n",
    "          'tuscola': 15,\n",
    "          'decatur': 5,\n",
    "          'island grove': 26,\n",
    "          'sumner': 10,\n",
    "          'oblong': 10,\n",
    "          'marshall': 2,\n",
    "          'paris': 3,\n",
    "          'danville': 2,\n",
    "          'fairfield': 16,\n",
    "          'mt carmel': 2,\n",
    "          'enfield': 8,\n",
    "          'harrisburg': 18,\n",
    "          'pleasant grove': 12,\n",
    "          'carbondale': 6,\n",
    "          'pickneyville': 15,\n",
    "          'hecker': 11,\n",
    "          'st jacob': 20,\n",
    "          'nokomis': 20\n",
    "         }\n",
    "date_range = ('2020-03-17', '2020-03-31') # 2 weeks total, starting 3 days before governor announced state shutdown. The 'Until' date is exclusive, so this range looks like 15 days\n",
    "state = 'il'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting 51.50286127007056 seconds before next iteration\n",
      "Finished current iteration for springfield, we got 1009 tweets.\n",
      "Waiting 51.50286127007056 seconds before next iteration\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to run\n",
    "state_tweets_to_csv(query, tweets_per_iter, cities, date_range, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_term_list = ['get', 'one', 'time', 'people', 'day', 'know', 'today', 'need', 'go', 'home', 'right', 'going', \n",
    "             'pandemic', 'coronavirus', 'news', 'health', 'covid', '19', 'quarantine', 'governor', 'capitol', 'capital', 'kemp', 'virus'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Georgia Query\n",
    "query = ' OR '.join(ga_term_list) # Joining with OR so that we get tweets that contain those individual words rather than phrases\n",
    "tweets_per_iter = 1000\n",
    "\n",
    "# Picking wider ranges for more rural areas, shallower ranges for cities, used google maps to try not to overlap, but we can also check for duplicates afterward.\n",
    "cities = {\n",
    "          'resaca': 19,\n",
    "          'east ellijay': 14,\n",
    "          'cleveland': 30,\n",
    "          'nicholson': 14,\n",
    "          'athens': 6,\n",
    "          'carlton': 17,\n",
    "          'philomath': 31,\n",
    "          'grovetown': 15,\n",
    "          'Herndon': 25,\n",
    "          'glennville': 20,\n",
    "          'georgetown': 7,\n",
    "          'brunswick': 6,\n",
    "          'atkinson': 15,\n",
    "          'sunnyside': 18,\n",
    "          'douglas': 25,\n",
    "          'tifton': 18,\n",
    "          'rebecca': 8,\n",
    "          'pineview': 12,\n",
    "          'warner robins': 10,\n",
    "          'macon': 4,\n",
    "          'mcintyre': 10,\n",
    "          'deepstep': 15,\n",
    "          'round oak': 15,\n",
    "          'jersey': 16,\n",
    "          'windsor': 10,\n",
    "          'cumming': 13,\n",
    "          'atlanta': 5,\n",
    "          'cartersville': 21,\n",
    "          'temple': 16,\n",
    "          'hogansville': 20,\n",
    "          'waverly hall': 15,\n",
    "          'buena vista': 10,\n",
    "          'shellman': 15,\n",
    "          'albany': 5,\n",
    "          'rowena': 12,\n",
    "          'branchville': 20,\n",
    "          'bainbridge': 10,\n",
    "          'pavo': 20,\n",
    "          'valdosta': 8,\n",
    "          'dublin': 5,\n",
    "          'alamo': 5\n",
    "         }\n",
    "date_range = ('2020-03-30', '2020-04-13') # 2 weeks total, starting 3 days before governor announced state shutdown. The 'Until' date is exclusive, so this range looks like 15 days\n",
    "state = 'ga'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to run\n",
    "state_tweets_to_csv(query, tweets_per_iter, cities, date_range, state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
